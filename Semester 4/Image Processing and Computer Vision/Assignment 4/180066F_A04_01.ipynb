{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 2761s 16us/step\n",
      "x_train:  (50000, 32, 32, 3)\n",
      "w1: (3072, 10)\n",
      "b1: (10,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "def unpickle(file): #If needed, for the downloaded CIFAR10 dataset\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data() #Loading data\n",
    "print(\"x_train: \", x_train.shape)\n",
    "\n",
    "K = len(np.unique(y_train)) #Classes\n",
    "Ntr = x_train.shape[0]\n",
    "Nte = x_test.shape[0]\n",
    "Din = 3072 #CIFAR10\n",
    "# Din = 784 # MINIST\n",
    "\n",
    "#Normalize pixel values\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "mean_image = np.mean(x_train, axis=0)\n",
    "x_train = x_train - mean_image\n",
    "x_test = x_test - mean_image\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=K)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=K)\n",
    "\n",
    "x_train = np.reshape(x_train,(Ntr,Din))\n",
    "x_test = np.reshape(x_test,(Nte,Din))\n",
    "x_train = x_train.astype(\"float32\")\n",
    "x_test = x_test.astype(\"float32\")\n",
    "\n",
    "std=1e-5 #To facilitate random samples\n",
    "w1 = std*np.random.randn(Din, K) #Initialization of weight matrix\n",
    "b1 = np.zeros(K) #Initialization of bias matrix\n",
    "print(\"w1:\", w1.shape)\n",
    "print(\"b1:\", b1.shape)\n",
    "batch_size = Ntr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rearranged x_train:  (50000, 3073)\n",
      "Rearranged w1:  (3073, 10)\n",
      "Gradient descent is started\n",
      "| Epoch 001 | Loss 0.500 | Tr Acc: 9.892 | Val Acc: 24.920 | LR: 0.014 |\n",
      "| Epoch 010 | Loss 0.456 | Tr Acc: 32.262 | Val Acc: 32.790 | LR: 0.014 |\n",
      "| Epoch 020 | Loss 0.440 | Tr Acc: 35.148 | Val Acc: 35.040 | LR: 0.014 |\n",
      "| Epoch 030 | Loss 0.430 | Tr Acc: 36.478 | Val Acc: 36.390 | LR: 0.014 |\n",
      "| Epoch 040 | Loss 0.422 | Tr Acc: 37.150 | Val Acc: 37.330 | LR: 0.013 |\n",
      "| Epoch 050 | Loss 0.417 | Tr Acc: 37.652 | Val Acc: 37.800 | LR: 0.013 |\n",
      "| Epoch 060 | Loss 0.413 | Tr Acc: 38.100 | Val Acc: 38.140 | LR: 0.013 |\n",
      "| Epoch 070 | Loss 0.409 | Tr Acc: 38.448 | Val Acc: 38.420 | LR: 0.013 |\n",
      "| Epoch 080 | Loss 0.407 | Tr Acc: 38.762 | Val Acc: 38.710 | LR: 0.013 |\n",
      "| Epoch 090 | Loss 0.405 | Tr Acc: 38.946 | Val Acc: 38.880 | LR: 0.013 |\n",
      "| Epoch 100 | Loss 0.403 | Tr Acc: 39.208 | Val Acc: 39.030 | LR: 0.013 |\n",
      "| Epoch 110 | Loss 0.402 | Tr Acc: 39.402 | Val Acc: 39.020 | LR: 0.013 |\n",
      "| Epoch 120 | Loss 0.401 | Tr Acc: 39.540 | Val Acc: 39.180 | LR: 0.012 |\n",
      "| Epoch 130 | Loss 0.400 | Tr Acc: 39.646 | Val Acc: 39.240 | LR: 0.012 |\n",
      "| Epoch 140 | Loss 0.399 | Tr Acc: 39.796 | Val Acc: 39.290 | LR: 0.012 |\n",
      "| Epoch 150 | Loss 0.399 | Tr Acc: 39.934 | Val Acc: 39.360 | LR: 0.012 |\n",
      "| Epoch 160 | Loss 0.398 | Tr Acc: 40.022 | Val Acc: 39.420 | LR: 0.012 |\n",
      "| Epoch 170 | Loss 0.398 | Tr Acc: 40.148 | Val Acc: 39.460 | LR: 0.012 |\n",
      "| Epoch 180 | Loss 0.397 | Tr Acc: 40.196 | Val Acc: 39.540 | LR: 0.012 |\n",
      "| Epoch 190 | Loss 0.397 | Tr Acc: 40.288 | Val Acc: 39.610 | LR: 0.012 |\n",
      "| Epoch 200 | Loss 0.397 | Tr Acc: 40.388 | Val Acc: 39.590 | LR: 0.011 |\n",
      "| Epoch 210 | Loss 0.396 | Tr Acc: 40.454 | Val Acc: 39.630 | LR: 0.011 |\n",
      "| Epoch 220 | Loss 0.396 | Tr Acc: 40.512 | Val Acc: 39.630 | LR: 0.011 |\n",
      "| Epoch 230 | Loss 0.396 | Tr Acc: 40.564 | Val Acc: 39.580 | LR: 0.011 |\n",
      "| Epoch 240 | Loss 0.396 | Tr Acc: 40.644 | Val Acc: 39.620 | LR: 0.011 |\n",
      "| Epoch 250 | Loss 0.395 | Tr Acc: 40.702 | Val Acc: 39.560 | LR: 0.011 |\n",
      "| Epoch 260 | Loss 0.395 | Tr Acc: 40.770 | Val Acc: 39.600 | LR: 0.011 |\n",
      "| Epoch 270 | Loss 0.395 | Tr Acc: 40.864 | Val Acc: 39.710 | LR: 0.011 |\n",
      "| Epoch 280 | Loss 0.395 | Tr Acc: 40.916 | Val Acc: 39.700 | LR: 0.011 |\n",
      "| Epoch 290 | Loss 0.395 | Tr Acc: 40.982 | Val Acc: 39.750 | LR: 0.010 |\n",
      "| Epoch 300 | Loss 0.395 | Tr Acc: 41.032 | Val Acc: 39.820 | LR: 0.010 |\n",
      "Gradient Descent is finished in order to train the parameters\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(hypo): #Sigmoid activation\n",
    "    return 1/(1+ np.exp(-hypo))\n",
    "\n",
    "def getAcc(predictions,labels): #Checking accuracy\n",
    "    pred_class = np.argmax(predictions, axis=1)\n",
    "    real_class = np.argmax(labels, axis=1)\n",
    "    valid_pred = [pred_class == real_class]\n",
    "    return 100*np.sum(valid_pred)/len(real_class)\n",
    "\n",
    "w1 = std*np.random.randn(Din, K) #To ensure that the weight matrix has not been changed due to repeated executions\n",
    "\n",
    "#Rearranging train and test samples:\n",
    "x_train_ra = np.concatenate((np.ones((x_train.shape[0],1)),x_train), axis=1); print('Rearranged x_train: ', x_train_ra.shape)\n",
    "x_test_ra  = np.concatenate((np.ones((x_test.shape[0],1)),x_test), axis=1)\n",
    "\n",
    "#Rearranging weight matrix and bias matrix into single matrix\n",
    "w1 = np.concatenate((b1.reshape(1,K), w1), axis=0); print('Rearranged w1: ',w1.shape)\n",
    "\n",
    "iterations = 300  #Interations of gradient descent\n",
    "lr = 1.4e-2 # Learning rate\n",
    "lr_decay= 0.999\n",
    "reg = 5e-6\n",
    "lr_hitory = []\n",
    "loss_history = [] #To store the values of loss function at each iteration \n",
    "train_acc_history = [] #To store the values of the training accuracies\n",
    "val_acc_history = [] #TO store the values of the validation accuracies\n",
    "\n",
    "m = x_train.shape[0]  #Number of training examples\n",
    "\n",
    "# Running gradient descent number of times speciied in iterations\n",
    "print(\"Gradient descent is started\") #In order to ensure the running process\n",
    "\n",
    "for t in range(1,iterations+1):    \n",
    "    # Forward Propagation\n",
    "    hypo = x_train_ra.dot(w1)\n",
    "    loss = (1/(2*m))*np.sum(( hypo - y_train)**2) + (1/(2*m))*reg*np.sum(w1**2) \n",
    "    loss_history.append(loss)\n",
    "    \n",
    "    # Backward Propagation\n",
    "    dw1 = (1/m)*(x_train_ra.T.dot(hypo - y_train))  + (1/m)*reg*w1 \n",
    "    w1 = w1 - lr*dw1\n",
    "    \n",
    "    # Training Accuracy and Validation Accuracy\n",
    "    train_acc = getAcc(hypo, y_train)\n",
    "    train_acc_history.append(train_acc)\n",
    "    valid_acc = getAcc(x_test_ra.dot(w1), y_test)\n",
    "    val_acc_history.append(valid_acc)\n",
    "    \n",
    "    # Print details for selected iterations\n",
    "    if (t%10==0) or (t==1):\n",
    "        print(\"| Epoch {:03} | Loss {:.3f} | Tr Acc: {:.3f} | Val Acc: {:.3f} | LR: {:.3f} |\"\\\n",
    "             .format(t,loss,train_acc,valid_acc,lr))\n",
    "    \n",
    "    # Decaying learning rate\n",
    "    lr_hitory.append(lr)\n",
    "    lr = lr*lr_decay\n",
    "    \n",
    "print(\"Gradient Descent is finished in order to train the parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting learning rate, training and testing loss and accuracies-------\n",
    "fig, axes  = plt.subplots(1,4, sharex='all', sharey='all', figsize=(50,10))\n",
    "items = {\"Loss\":loss_history, \"Training Accuracy\":train_acc_history,\\\n",
    "         \"Validation Accuracy\": val_acc_history, \"Learning Rate\":lr_hitory}\n",
    "location = 1\n",
    "for key in items.keys():\n",
    "    plt.subplot(1,4,location)\n",
    "    plt.plot(items[key], color='#EF0C1E', linewidth=4)\n",
    "    plt.title(key)\n",
    "    location+=1\n",
    "plt.show()\n",
    "\n",
    "#Showing the weights matrix W1 as 10 images\n",
    "weights = w1[1:,] #To remove the row of bias terms\n",
    "weights_pos =  weights- np.min(weights) #TO ensure that the minimum weight is zero\n",
    "images = ((weights_pos/np.max(weights_pos))*255).astype('uint8')\n",
    "CIFAR10 = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "fig, axes  = plt.subplots(2,5, sharex='all', sharey='all', figsize=(25,10))\n",
    "location = 1 #Location of the image in the grid of 2x5\n",
    "for i in range(K):\n",
    "    image = images[:,i].reshape(32,32,3)\n",
    "    plt.subplot(2,5,location),plt.imshow(image[:,:,::-1])\n",
    "    plt.title(\"Class: {}\".format(CIFAR10[i])),plt.xticks([]),plt.yticks([])\n",
    "    location+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
